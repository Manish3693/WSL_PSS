{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the Graph\n",
    "\n",
    "`init_graph` : this function just makes the structure of the graph that will represent the talukas. Also, there was some trouble in the Adjecency file(on the 81th taluka), so I simply created the graph, and removed the 81th index taluka(which wasn't there in the 1st place)\n",
    "\n",
    "\n",
    "`init_graph_attr` : Here, we are just initializing the graph, with capability vector, taluka name, and stress = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_graph(G,node_adj_frame):\n",
    "    G.add_nodes_from([i for i in range(len(node_adj_frame))])\n",
    "    labels = {}\n",
    "    labels = node_adj_frame.columns\n",
    "    for i in range(len(node_adj_frame)):\n",
    "        snode = node_adj_frame[labels[0]][i]-1\n",
    "        if snode == 80:\n",
    "            continue\n",
    "        temp = node_adj_frame[labels[2]][i]\n",
    "        if ',' in str(temp):\n",
    "            sedge_arr = temp.split(',')\n",
    "            # removed the if condition of k = 81\n",
    "            for j in range(0, len(sedge_arr)):\n",
    "                k = int(sedge_arr[j])\n",
    "                G.add_edge(snode, k-1)\n",
    "        elif np.isnan(temp):\n",
    "            print(\"ERROR: Not found in the adjacency excel sheet\")\n",
    "        else:\n",
    "            G.add_edge(snode, int(temp)-1)\n",
    "    G.remove_node(80) #removed the extra node made during graph creation\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_graph_attr(G,AdjFile,df,col1, col2):\n",
    "    node_adj_frame = pd.read_excel(AdjFile)\n",
    "    node_list = node_adj_frame[\"KGISTalukN\"].tolist()\n",
    "    node_list.insert(80, \"\")\n",
    "    nodeAttr = {}\n",
    "    init_graph(G,node_adj_frame)\n",
    "    capability_vector = list(zip(df[col1], df[col2]))\n",
    "    node_attri_dict = dict(zip(df[\"Taluka\"],capability_vector))\n",
    "    node_attri_dict = dict((k.lower(), v) for k, v in node_attri_dict.items())\n",
    "    for i in range(len(node_adj_frame)):\n",
    "        temp = {}\n",
    "        if i == 80:\n",
    "            continue\n",
    "        temp[\"capabilityvector\"] = node_attri_dict[node_list[i].lower()]\n",
    "        temp[\"nodeStress\"] = 0\n",
    "        temp[\"name\"] = node_list[i]\n",
    "        nodeAttr[i] = temp\n",
    "    # explicitly added the last taluka from the adj. list\n",
    "    nt = {}\n",
    "    nt[\"capabilityvector\"] = node_attri_dict[node_list[226].lower()]\n",
    "    nt[\"nodeStress\"] = 0\n",
    "    nt[\"name\"] = \"Hadagali\"\n",
    "    nodeAttr[226] = nt\n",
    "    nx.set_node_attributes(G, nodeAttr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the graph from networkx library.\n",
    "G = nx.Graph()\n",
    "df = pd.read_excel('../input_files/Combined_PIA_IMR_MMR.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this dictionary will be having District and a list of talukas inside them.\n",
    "dist_taluka_dict = defaultdict(list)\n",
    "for k, v in zip(df[\"District_GIS\"], df[\"Taluka\"]):\n",
    "    dist_taluka_dict[k].append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addList(l1,l2):\n",
    "    for i in range(len(l1)):\n",
    "        l1[i] = l1[i] + l2[i]\n",
    "    return l1\n",
    "def divList(l1,k):\n",
    "    for i in range(len(l1)):\n",
    "        l1[i] = l1[i]/k\n",
    "    return l1\n",
    "def l2_normalization(l1,l2):\n",
    "    k = 0\n",
    "    for i in range(len(l1)):\n",
    "        k+= (l1[i] - l2[i])**2\n",
    "    return math.sqrt(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Stress\n",
    "\n",
    "Here, we are calculating stress. These are the steps:\n",
    "1. Calculate the centeroid for each node, that will be the average of the neighbors of elements of the capability vector. Now, here, we are taking the centroid as the sum of those values, and not average. We will average them at the time of calculation.\n",
    "2. Now, take the l2 distance between the capability vector of the node and it's centroid. This will be the stress. Also, note that stability is just 1-stress for the taluka, and that is the only difference between get_node_stress and get_node_stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_stress(G):\n",
    "    taluka_stress_dict = {}\n",
    "    for n in G.nodes():\n",
    "        centroid = [0,0]\n",
    "        neighList = list(G.neighbors(n))\n",
    "        for nei in neighList:\n",
    "            try:\n",
    "                centroid = addList(centroid,list(G.nodes[nei][\"capabilityvector\"]))\n",
    "            except(KeyError):\n",
    "                pass\n",
    "        try:\n",
    "            G.nodes[n][\"nodeStress\"] = l2_normalization(divList(centroid,len(neighList)),list(G.nodes[n][\"capabilityvector\"]))\n",
    "        except(KeyError):\n",
    "            pass\n",
    "        try:\n",
    "            taluka_stress_dict[G.nodes[n][\"name\"].lower()]=G.nodes[n][\"nodeStress\"]\n",
    "        except(KeyError):\n",
    "            pass\n",
    "    return taluka_stress_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_stability(G):\n",
    "    taluka_stress_dict = {}\n",
    "    for n in G.nodes():\n",
    "        centroid = [0,0]\n",
    "        neighList = list(G.neighbors(n))\n",
    "        for nei in neighList:\n",
    "            try:\n",
    "                centroid = addList(centroid,list(G.nodes[nei][\"capabilityvector\"]))\n",
    "            except(KeyError):\n",
    "                pass\n",
    "        try:\n",
    "            G.nodes[n][\"nodeStress\"] = 1 - l2_normalization(divList(centroid,len(neighList)),list(G.nodes[n][\"capabilityvector\"]))\n",
    "        except(KeyError):\n",
    "            pass\n",
    "        try:\n",
    "            taluka_stress_dict[G.nodes[n][\"name\"].lower()]=G.nodes[n][\"nodeStress\"]\n",
    "        except(KeyError):\n",
    "            pass\n",
    "    return taluka_stress_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating initial stress before any intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Taluka'] = df['Taluka'].str.lower()\n",
    "init_graph_attr(G, '../input_files/IMR_Stress_AdjFile.xlsx', df, \"Normalized MMR\", \"Normalized IMR\")\n",
    "initialstress = get_node_stress(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Initial Stress\"] = df[\"Taluka\"].map(initialstress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stress Calculation after ANC -20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_graph_attr(G, '../input_files/IMR_Stress_AdjFile.xlsx', df, \"Normalized IMR (ANC-20%)\", \"Normalized MMR (ANC -20%)\")\n",
    "ANCminus20stress = get_node_stress(G)\n",
    "df[\"Stress(ANC - 20%)\"] = df[\"Taluka\"].map(ANCminus20stress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stress Calculation after ANC +20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_graph_attr(G, '../input_files/IMR_Stress_AdjFile.xlsx', df, \"Normalized IMR (ANC+20%)\", \"Normalized MMR (ANC +20%)\")\n",
    "ANCplus20stress = get_node_stress(G)\n",
    "df[\"Stress(ANC + 20%)\"] = df[\"Taluka\"].map(ANCplus20stress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stress Calculation after ANC -10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_graph_attr(G, '../input_files/IMR_Stress_AdjFile.xlsx', df, \"Normalized IMR (ANC-10%)\", \"Normalized MMR (ANC -10%)\")\n",
    "ANCminus10stress = get_node_stress(G)\n",
    "df[\"Stress(ANC - 10%)\"] = df[\"Taluka\"].map(ANCminus10stress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stress Calculation after ANC +10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_graph_attr(G, '../input_files/IMR_Stress_AdjFile.xlsx', df, \"Normalized IMR (ANC+10%)\", \"Normalized MMR (ANC +10%)\")\n",
    "ANCplus10stress = get_node_stress(G)\n",
    "df[\"Stress(ANC + 10%)\"] = df[\"Taluka\"].map(ANCplus10stress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.936246\n",
       "1      0.854066\n",
       "2      0.882655\n",
       "3      0.587826\n",
       "4      0.621669\n",
       "         ...   \n",
       "221    0.641837\n",
       "222    0.803416\n",
       "223    0.534068\n",
       "224    0.712948\n",
       "225    0.868064\n",
       "Name: Normalized IMR (ANC+10%), Length: 226, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Normalized IMR (ANC+10%)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation\n",
    "\n",
    "The aggregate function simple takes the average of all the talukas in a district and maps them to the corresponding district. This function outputs a dictionary with key: district name and value: value to be aggregated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(taluka_dict):\n",
    "    dist_stress = {}\n",
    "    for dist, taluks in dist_taluka_dict.items():\n",
    "        agg_stress = 0\n",
    "        for taluk in taluks:\n",
    "            try:\n",
    "                agg_stress = agg_stress + taluka_dict[taluk.lower()]\n",
    "            except(KeyError):\n",
    "                pass\n",
    "        dist_stress[dist] = agg_stress/len(taluks)\n",
    "    return dist_stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp_IMR = pd.read_excel(\"../input_files/PIA_Normalized_IMR.xlsx\")\n",
    "df_imp_MMR = pd.read_excel(\"../input_files/PIA_Normalized_MMR.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_Impact_ANC = pd.DataFrame() #it will have taluka stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_df = pd.DataFrame() #initialized dataframe for district level calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Stress\n",
    "\n",
    "In the next 8 steps, I am just calculating the stress values for different values of ANC. Now, It can be put in a for loop, but the column names are not in any pattern that will give an easy for loop. Also, the column names might change in the future for the PIA file. So, that's why I did a little bit of brute forcing.\n",
    "\n",
    "\n",
    "Here, will will get the taluka level stress in combined_Impact_ANC and district level stress in aggregate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================FOR ANC -20%==============================================================\n",
    "# Getting the graph ready for ANC -20%\n",
    "init_graph_attr(G, '../input_files/IMR_Stress_AdjFile.xlsx', df, \"Normalized IMR (ANC-20%)\", \"Normalized MMR (ANC -20%)\")\n",
    "\n",
    "# These dict have the taluka names and Impact Score for ANC -20%\n",
    "IMR_impact_dict = dict(zip(df_imp_IMR[\"Taluka\"],df_imp_IMR[\"Impact Score (ANC -20%)\"]))\n",
    "MMR_impact_dict = dict(zip(df_imp_MMR[\"Taluka\"],df_imp_MMR[\"Impact Score (ANC -20%)\"]))\n",
    "\n",
    "# Here, we are just converting the taluka names in these dictionaries to lower case\n",
    "IMR_impact_dict= dict((k.lower(), v) for k, v in IMR_impact_dict.items())\n",
    "MMR_impact_dict= dict((k.lower(), v) for k, v in MMR_impact_dict.items())\n",
    "\n",
    "# Aggregating IMR and MMR impact scores\n",
    "aggregate_IMR_Impact = aggregate(IMR_impact_dict)\n",
    "aggregate_MMR_Impact = aggregate(MMR_impact_dict)\n",
    "\n",
    "# Aggregating the stress for \n",
    "aggregate_Stress = aggregate(get_node_stress(G))\n",
    "\n",
    "# Putting it in a temp df\n",
    "temp2_df = pd.DataFrame.from_dict([aggregate_IMR_Impact, aggregate_MMR_Impact, aggregate_Stress])\n",
    "aI_df = temp2_df.T\n",
    "aI_df = temp2_df.transpose()\n",
    "aI_df.rename(columns = {0:'IMPACT_SCORE_IMR (ANC - 20%)',1:'IMPACT_SCORE_MMR (ANC - 20%)', 2:'STRESS_SCORE (ANC - 20%)'}, inplace = True)\n",
    "aggregate_df = aI_df\n",
    "\n",
    "# Now, we are converting them into a dataframe and making them Taluka, Impact, Stress\n",
    "combined_IMR = pd.DataFrame.from_dict([IMR_impact_dict, get_node_stress(G)])\n",
    "combined_MMR = pd.DataFrame.from_dict([MMR_impact_dict, get_node_stress(G)])\n",
    "\n",
    "# Now, we are taking a transpose, so that we get it in column form\n",
    "trdIMR = combined_IMR.T\n",
    "trdMMR = combined_MMR.T\n",
    "trdIMR = combined_IMR.transpose()\n",
    "trdMMR = combined_MMR.transpose()\n",
    "\n",
    "# Renaming the columns\n",
    "trdIMR.rename(columns = {0:'IMPACT_SCORE_IMR (ANC - 20%)', 1:'STRESS_SCORE (ANC - 20%)'}, inplace = True)\n",
    "trdMMR.rename(columns = {0:'IMPACT_SCORE_MMR (ANC - 20%)', 1:'STRESS_SCORE (ANC - 20%)'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_Impact_ANC = trdIMR\n",
    "combined_Impact_ANC['IMPACT_SCORE_MMR (ANC - 20%)'] = trdMMR['IMPACT_SCORE_MMR (ANC - 20%)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================FOR ANC -10%==============================================================\n",
    "# Getting the graph ready for ANC -10%\n",
    "init_graph_attr(G, '../input_files/IMR_Stress_AdjFile.xlsx', df, \"Normalized IMR (ANC-10%)\", \"Normalized MMR (ANC -10%)\")\n",
    "\n",
    "# These dict have the names vs Impact Score for ANC -10%\n",
    "IMR_impact_dict = dict(zip(df_imp_IMR[\"Taluka\"],df_imp_IMR[\"Impact Score (ANC -10%)\"]))\n",
    "MMR_impact_dict = dict(zip(df_imp_MMR[\"Taluka\"],df_imp_MMR[\"Impact_____Deprivation % - Households with with any woman has not received at least 4 antenatal care visits for the most recent birth or has not received assistance from trained skilled medical personnel during the most recent childbirth._____-10%\"]))\n",
    "\n",
    "# Here, we are just converting the taluka names in these dictionaries to lower case\n",
    "IMR_impact_dict= dict((k.lower(), v) for k, v in IMR_impact_dict.items())\n",
    "MMR_impact_dict= dict((k.lower(), v) for k, v in MMR_impact_dict.items())\n",
    "\n",
    "# Aggregating both the impacts\n",
    "aggregate_IMR_Impact = aggregate(IMR_impact_dict)\n",
    "aggregate_MMR_Impact = aggregate(MMR_impact_dict)\n",
    "\n",
    "# Aggregating the stress\n",
    "aggregate_Stress = aggregate(get_node_stress(G))\n",
    "\n",
    "# Putting it in a temp df\n",
    "temp2_df = pd.DataFrame.from_dict([aggregate_IMR_Impact, aggregate_MMR_Impact, aggregate_Stress])\n",
    "aI_df = temp2_df.T\n",
    "aI_df = temp2_df.transpose()\n",
    "aI_df.rename(columns = {0:'IMPACT_SCORE_IMR (ANC - 10%)',1:'IMPACT_SCORE_MMR (ANC - 10%)', 2:'STRESS_SCORE (ANC -10%)'}, inplace = True)\n",
    "aggregate_df['IMPACT_SCORE_IMR (ANC - 10%)'] = aI_df['IMPACT_SCORE_IMR (ANC - 10%)']\n",
    "aggregate_df['IMPACT_SCORE_MMR (ANC - 10%)'] = aI_df['IMPACT_SCORE_MMR (ANC - 10%)']\n",
    "aggregate_df['STRESS_SCORE (ANC -10%)'] = aI_df['STRESS_SCORE (ANC -10%)']\n",
    "\n",
    "# Now, we are converting them into a dataframe and making them Taluka, Impact, Stress\n",
    "combined_IMR = pd.DataFrame.from_dict([IMR_impact_dict, get_node_stress(G)])\n",
    "combined_MMR = pd.DataFrame.from_dict([MMR_impact_dict, get_node_stress(G)])\n",
    "\n",
    "# Now, we are taking a transpose, so that we get it in column form\n",
    "trdIMR = combined_IMR.T\n",
    "trdMMR = combined_MMR.T\n",
    "trdIMR = combined_IMR.transpose()\n",
    "trdMMR = combined_MMR.transpose()\n",
    "\n",
    "# Renaming the columns\n",
    "trdIMR.rename(columns = {0:'IMPACT_SCORE_IMR (ANC - 10%)', 1:'STRESS_SCORE (ANC -10%)'}, inplace = True)\n",
    "trdMMR.rename(columns = {0:'IMPACT_SCORE_MMR (ANC - 10%)', 1:'STRESS_SCORE (ANC -10%)'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_Impact_ANC['IMPACT_SCORE_MMR (ANC - 10%)'] = trdMMR['IMPACT_SCORE_MMR (ANC - 10%)']\n",
    "combined_Impact_ANC['IMPACT_SCORE_IMR (ANC - 10%)'] = trdIMR['IMPACT_SCORE_IMR (ANC - 10%)']\n",
    "combined_Impact_ANC['STRESS_SCORE (ANC - 10%)'] = trdMMR['STRESS_SCORE (ANC -10%)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================FOR ANC +10%==============================================================\n",
    "# Getting the graph ready for ANC +10%\n",
    "init_graph_attr(G, '../input_files/IMR_Stress_AdjFile.xlsx', df, \"Normalized IMR (ANC+10%)\", \"Normalized MMR (ANC +10%)\")\n",
    "\n",
    "# These dict have the names vs Impact Score for ANC +10%\n",
    "IMR_impact_dict = dict(zip(df_imp_IMR[\"Taluka\"],df_imp_IMR[\"Impact Score (ANC +10%)\"]))\n",
    "MMR_impact_dict = dict(zip(df_imp_MMR[\"Taluka\"],df_imp_MMR[\"Impact_____Deprivation % - Households with with any woman has not received at least 4 antenatal care visits for the most recent birth or has not received assistance from trained skilled medical personnel during the most recent childbirth._____+10%\"]))\n",
    "\n",
    "# Here, we are just converting the taluka names in these dictionaries to lower case\n",
    "IMR_impact_dict= dict((k.lower(), v) for k, v in IMR_impact_dict.items())\n",
    "MMR_impact_dict= dict((k.lower(), v) for k, v in MMR_impact_dict.items())\n",
    "\n",
    "# Aggregating both the impacts\n",
    "aggregate_IMR_Impact = aggregate(IMR_impact_dict)\n",
    "aggregate_MMR_Impact = aggregate(MMR_impact_dict)\n",
    "\n",
    "# Aggregating the stress\n",
    "aggregate_Stress = aggregate(get_node_stress(G))\n",
    "\n",
    "# Putting it in a temp df\n",
    "temp2_df = pd.DataFrame.from_dict([aggregate_IMR_Impact, aggregate_MMR_Impact, aggregate_Stress])\n",
    "aI_df = temp2_df.T\n",
    "aI_df = temp2_df.transpose()\n",
    "aI_df.rename(columns = {0:'IMPACT_SCORE_IMR (ANC + 10%)',1:'IMPACT_SCORE_MMR (ANC + 10%)', 2:'STRESS_SCORE (ANC +10%)'}, inplace = True)\n",
    "aggregate_df['IMPACT_SCORE_IMR (ANC + 10%)'] = aI_df['IMPACT_SCORE_IMR (ANC + 10%)']\n",
    "aggregate_df['IMPACT_SCORE_MMR (ANC + 10%)'] = aI_df['IMPACT_SCORE_MMR (ANC + 10%)']\n",
    "aggregate_df['STRESS_SCORE (ANC +10%)'] = aI_df['STRESS_SCORE (ANC +10%)']\n",
    "\n",
    "# Now, we are converting them into a dataframe and making them Taluka, Impact, Stress\n",
    "combined_IMR = pd.DataFrame.from_dict([IMR_impact_dict, get_node_stress(G)])\n",
    "combined_MMR = pd.DataFrame.from_dict([MMR_impact_dict, get_node_stress(G)])\n",
    "\n",
    "# Now, we are taking a transpose, so that we get it in column form\n",
    "trdIMR = combined_IMR.T\n",
    "trdMMR = combined_MMR.T\n",
    "trdIMR = combined_IMR.transpose()\n",
    "trdMMR = combined_MMR.transpose()\n",
    "\n",
    "# Renaming the columns\n",
    "trdIMR.rename(columns = {0:'IMPACT_SCORE_IMR (ANC + 10%)', 1:'STRESS_SCORE (ANC +10%)'}, inplace = True)\n",
    "trdMMR.rename(columns = {0:'IMPACT_SCORE_MMR (ANC + 10%)', 1:'STRESS_SCORE (ANC +10%)'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_Impact_ANC['IMPACT_SCORE_MMR (ANC + 10%)'] = trdMMR['IMPACT_SCORE_MMR (ANC + 10%)']\n",
    "combined_Impact_ANC['IMPACT_SCORE_IMR (ANC + 10%)'] = trdIMR['IMPACT_SCORE_IMR (ANC + 10%)']\n",
    "combined_Impact_ANC['STRESS_SCORE (ANC + 10%)'] = trdMMR['STRESS_SCORE (ANC +10%)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================FOR ANC +20%==============================================================\n",
    "# Getting the graph ready for ANC -10%\n",
    "init_graph_attr(G, '../input_files/IMR_Stress_AdjFile.xlsx', df, \"Normalized IMR (ANC+20%)\", \"Normalized MMR (ANC +20%)\")\n",
    "\n",
    "# These dict have the names vs Impact Score for ANC +20%\n",
    "IMR_impact_dict = dict(zip(df_imp_IMR[\"Taluka\"],df_imp_IMR[\"Impact Score (ANC +20%)\"]))\n",
    "MMR_impact_dict = dict(zip(df_imp_MMR[\"Taluka\"],df_imp_MMR[\"Impact_____Deprivation % - Households with with any woman has not received at least 4 antenatal care visits for the most recent birth or has not received assistance from trained skilled medical personnel during the most recent childbirth._____+20%\"]))\n",
    "\n",
    "# Here, we are just converting the taluka names in these dictionaries to lower case\n",
    "IMR_impact_dict= dict((k.lower(), v) for k, v in IMR_impact_dict.items())\n",
    "MMR_impact_dict= dict((k.lower(), v) for k, v in MMR_impact_dict.items())\n",
    "\n",
    "# Aggregating both the impacts\n",
    "aggregate_IMR_Impact = aggregate(IMR_impact_dict)\n",
    "aggregate_MMR_Impact = aggregate(MMR_impact_dict)\n",
    "\n",
    "# Aggregating the stress\n",
    "aggregate_Stress = aggregate(get_node_stress(G))\n",
    "\n",
    "# Putting it in a temp df\n",
    "temp2_df = pd.DataFrame.from_dict([aggregate_IMR_Impact, aggregate_MMR_Impact, aggregate_Stress])\n",
    "aI_df = temp2_df.T\n",
    "aI_df = temp2_df.transpose()\n",
    "aI_df.rename(columns = {0:'IMPACT_SCORE_IMR (ANC + 20%)',1:'IMPACT_SCORE_MMR (ANC + 20%)', 2:'STRESS_SCORE (ANC +20%)'}, inplace = True)\n",
    "aggregate_df['IMPACT_SCORE_IMR (ANC + 20%)'] = aI_df['IMPACT_SCORE_IMR (ANC + 20%)']\n",
    "aggregate_df['IMPACT_SCORE_MMR (ANC + 20%)'] = aI_df['IMPACT_SCORE_MMR (ANC + 20%)']\n",
    "aggregate_df['STRESS_SCORE (ANC +20%)'] = aI_df['STRESS_SCORE (ANC +20%)']\n",
    "\n",
    "# Now, we are converting them into a dataframe and making them Taluka, Impact, Stress\n",
    "combined_IMR = pd.DataFrame.from_dict([IMR_impact_dict, get_node_stress(G)])\n",
    "combined_MMR = pd.DataFrame.from_dict([MMR_impact_dict, get_node_stress(G)])\n",
    "\n",
    "# Now, we are taking a transpose, so that we get it in column form\n",
    "trdIMR = combined_IMR.T\n",
    "trdMMR = combined_MMR.T\n",
    "trdIMR = combined_IMR.transpose()\n",
    "trdMMR = combined_MMR.transpose()\n",
    "\n",
    "# Renaming the columns\n",
    "trdIMR.rename(columns = {0:'IMPACT_SCORE_IMR (ANC + 20%)', 1:'STRESS_SCORE (ANC +20%)'}, inplace = True)\n",
    "trdMMR.rename(columns = {0:'IMPACT_SCORE_MMR (ANC + 20%)', 1:'STRESS_SCORE (ANC +20%)'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_Impact_ANC['IMPACT_SCORE_MMR (ANC + 20%)'] = trdMMR['IMPACT_SCORE_MMR (ANC + 20%)']\n",
    "combined_Impact_ANC['IMPACT_SCORE_IMR (ANC + 20%)'] = trdIMR['IMPACT_SCORE_IMR (ANC + 20%)']\n",
    "combined_Impact_ANC['STRESS_SCORE (ANC + 20%)'] = trdMMR['STRESS_SCORE (ANC +20%)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_Impact_ANC.to_excel('../output_files_2d/2D_talukaLevel_stress_impact.xlsx')\n",
    "aggregate_df.to_excel('../output_files_2d/2D_districtLevel_stress_impact.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sustainable Intervention Score Calculations\n",
    "\n",
    "The formula for calculating the score is:  $$score = (impact_{avg} * stability) \\over dissonance $$\n",
    "\n",
    "`district_SI_score`: it calculates the score of the districts. We pass empty lists in it and this function populates them with values. It uses the aggregated values of stress and impacts.\n",
    "\n",
    "\n",
    "`taluka_SI_score`: it calculates the score of the talukas. We pass empty lists in it and this function populates them with values. It uses the taluka level values of stress and impacts.\n",
    "\n",
    "`normailize`: takes a dataframe ,column name, empty list, the upper limit and lower limit of normalization values and returna normalized. Let the normalization range be [a, b], and, X be the column, then, the formula used for normalization is: $$X_{normalized}[i] = a + (b - a) * { (X_{original}[i] - max(X_{original})) \\over (max(X_{original}) - mix(X_{original}))}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score = pd.DataFrame()\n",
    "df_score['Taluka'] = df['Taluka']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_temp = ['+2', '+1', '-1', '- 2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def district_SI_score(df, score_list, i, diss_list, imr_impact, mmr_impact, avg_impact):\n",
    "    for j in range(len(df)):\n",
    "        str_imr = 'IMPACT_SCORE_IMR (ANC ' + i + '0%)'\n",
    "        str_mmr = 'IMPACT_SCORE_MMR (ANC ' + i + '0%)'\n",
    "        str_stress = 'STRESS_SCORE (ANC ' + i + '0%)'\n",
    "\n",
    "        imr_impact.append(df[str_imr][j])\n",
    "        mmr_impact.append(df[str_mmr][j])\n",
    "        mean_impact = (df[str_imr][j] + df[str_mmr][j]) / 2 #mean impact of the taluka\n",
    "        avg_impact.append(mean_impact)\n",
    "        dissonance = abs(df[str_imr][j] - df[str_mmr][j]) #dissonance between the taluka\n",
    "        diss_list.append(dissonance)\n",
    "        score = (mean_impact * (1 - df[str_stress][j])) / dissonance\n",
    "        score_list.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taluka_SI_score(df, score_list, i):\n",
    "    for j in range(len(df)):\n",
    "        str_imr = 'IMPACT_SCORE_IMR (ANC ' + i + '0%)'\n",
    "        str_mmr = 'IMPACT_SCORE_MMR (ANC ' + i + '0%)'\n",
    "        str_stress = 'STRESS_SCORE (ANC ' + i + '0%)'\n",
    "\n",
    "        mean_impact = (df[str_imr][j] + df[str_mmr][j]) / 2 #mean impact of the taluka\n",
    "\n",
    "        dissonance = abs(df[str_imr][j] - df[str_mmr][j]) #dissonance between the taluka\n",
    "\n",
    "        score = (mean_impact * (1 - df[str_stress][j])) / dissonance\n",
    "        score_list.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = ['+ 2', '+ 1', '- 1', '- 2'] #for automation of all interventions\n",
    "for i in temp:\n",
    "    score_list = []\n",
    "    taluka_SI_score(combined_Impact_ANC, score_list, i)\n",
    "    str = 'SI_Score (ANC ' + i + '0%)'\n",
    "    df_score[str] = score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score.to_excel('../output_files_2d/2D_talukaLevel_SI_scores.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating the SI_Scores at district level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_score_df = pd.DataFrame()\n",
    "aggregated_score_df['District'] = dist_taluka_dict.keys() #adding new district column\n",
    "# aggregated_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df, col_str, nd_list, lower_lim, upper_lim):\n",
    "    for j in range(len(df)):\n",
    "        norm_dis = 0\n",
    "        norm_dis = lower_lim + (upper_lim - lower_lim)*(df[col_str][j] - min(df[col_str]))/(max(df[col_str]) - min(df[col_str]))\n",
    "        nd_list.append(norm_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_df.rename(columns={'STRESS_SCORE (ANC +10%)':'STRESS_SCORE (ANC + 10%)'}, inplace=True)\n",
    "aggregate_df.rename(columns={'STRESS_SCORE (ANC +20%)':'STRESS_SCORE (ANC + 20%)'}, inplace=True)\n",
    "aggregate_df.rename(columns={'STRESS_SCORE (ANC -10%)':'STRESS_SCORE (ANC - 10%)'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in temp:\n",
    "    score_list = []\n",
    "    diss_list = []\n",
    "    imr_impact = []\n",
    "    mmr_impact = []\n",
    "    avg_impact = []\n",
    "    district_SI_score(aggregate_df, score_list, i, diss_list, imr_impact, mmr_impact, avg_impact)\n",
    "    score_str = 'SI_Score (ANC ' + i + '0%)'\n",
    "    str_imr = 'IMPACT_SCORE_IMR (ANC ' + i + '0%)'\n",
    "    str_mmr = 'IMPACT_SCORE_MMR (ANC ' + i + '0%)'\n",
    "    str_diss = 'DISSONANCE (ANC ' + i + '0%)'\n",
    "    str_avg_imr = 'MEAN IMR (ANC ' + i + '0%)'\n",
    "    aggregated_score_df[score_str] = score_list\n",
    "    aggregated_score_df[str_imr] = imr_impact\n",
    "    aggregated_score_df[str_mmr] = mmr_impact\n",
    "    aggregated_score_df[str_avg_imr] = avg_impact\n",
    "    aggregated_score_df[str_diss] = diss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here, we are saving the raw sacores\n",
    "aggregated_score_df.to_excel('../output_files_2d/2D_districtLevel_SI_scores.xlsx') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in temp:\n",
    "    normalized_score_list = []\n",
    "    score_str = 'SI_Score (ANC ' + i + '0%)'\n",
    "    normalize(aggregated_score_df, score_str, normalized_score_list, 0, 1)\n",
    "    aggregated_score_df[score_str] = normalized_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here, we are saving the normalized scores.\n",
    "aggregated_score_df.to_excel('../output_files_2d/2D_districtLevel_normalized_SI_scores.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
